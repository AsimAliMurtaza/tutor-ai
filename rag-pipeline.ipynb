{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = ''\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + ' '\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize the extracted text.\"\"\"\n",
    "    text = re.sub(\n",
    "        r'\\s+', ' ', text)  \n",
    "    text = re.sub(r'[^\\w\\s.,;!?]', '', text)\n",
    "    return text.lower().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"Split the text into smaller chunks of fixed size.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size])\n",
    "              for i in range(0, len(words), chunk_size)]\n",
    "    return chunks if chunks else [\"No meaningful text found.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks):\n",
    "    \"\"\"Generate embeddings for text chunks using a pre-trained model.\"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Create a FAISS index for the embeddings.\"\"\"\n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        raise ValueError(\n",
    "            \"Embeddings are empty. Ensure text extraction is successful.\")\n",
    "\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    if len(embeddings.shape) == 1:\n",
    "        embeddings = embeddings.reshape(1, -1)  # Convert (N,) to (1, N)\n",
    "\n",
    "    dimension = embeddings.shape[1]  # Extract the embedding dimension\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity search\n",
    "    index.add(embeddings)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_knowledge_base(index, chunks, index_path, chunks_path):\n",
    "    \"\"\"Save the FAISS index and text chunks to disk.\"\"\"\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_knowledge_base(index_path, chunks_path):\n",
    "    \"\"\"Load the FAISS index and text chunks from disk.\"\"\"\n",
    "    if not os.path.exists(index_path) or not os.path.exists(chunks_path):\n",
    "        raise FileNotFoundError(\"Knowledge base files are missing.\")\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = f.read().splitlines()\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, index, chunks, top_k):\n",
    "    \"\"\"Retrieve the top-k most relevant chunks for a given query.\"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    relevant_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "\n",
    "    return relevant_chunks if relevant_chunks else [\"No relevant data found.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 9: Generate Response using a Pretrained Model\n",
    "\n",
    "# def generate_response(query, context):\n",
    "#     \"\"\"Generate a response based on retrieved chunks using a text generator model.\"\"\"\n",
    "\n",
    "#     # Load the text generation model\n",
    "#     generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "#     # Limit context length to avoid exceeding model's max length (GPT-2 has a 1024 token limit)\n",
    "#     max_context_length = 800  # Keeping some space for query & generated output\n",
    "#     truncated_context = context[:max_context_length]  # Truncate if needed\n",
    "\n",
    "#     # Construct the prompt\n",
    "#     prompt = f\"Answer the following question based on the context:\\n\\nContext: {truncated_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "#     # Generate response (fix: use max_new_tokens instead of max_length)\n",
    "#     response = generator(prompt, max_new_tokens=500, num_return_sequences=1)[\n",
    "#         0]['generated_text']\n",
    "\n",
    "#     return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(prompt):\n",
    "    for response in ollama.chat(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": prompt}], stream=True):\n",
    "        print(response[\"message\"][\"content\"], end=\"\", flush=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"/\"  \n",
    "index_path = \"knowledge_base.index\"\n",
    "chunks_path = \"knowledge_base.txt\"\n",
    "\n",
    "if not os.path.exists(pdf_folder):\n",
    "    raise FileNotFoundError(f\"PDF folder '{pdf_folder}' not found.\")\n",
    "\n",
    "pdf_file = open('book.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "text_data = []\n",
    "for page in pdf_reader.pages[32:]:\n",
    "    text_data.append(page.extract_text())\n",
    "\n",
    "pdf_file.close()\n",
    "\n",
    "full_text = \" \".join(text_data)\n",
    "if not full_text.strip():\n",
    "    raise ValueError(\n",
    "        \"No text extracted from PDFs. Check if files contain selectable text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(full_text)\n",
    "chunks = chunk_text(cleaned_text, chunk_size=500)\n",
    "embeddings = generate_embeddings(chunks)\n",
    "print(\n",
    "    f\"Generated {len(embeddings)} embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "index = create_faiss_index(embeddings)\n",
    "save_knowledge_base(index, chunks, index_path, chunks_path)\n",
    "index, chunks = load_knowledge_base(index_path, chunks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"\\nEnter your query (or type 'exit' to quit): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, index, chunks, top_k=3)\n",
    "    print(\"\\nRelevant Chunks:\")\n",
    "    for i, chunk in enumerate(relevant_chunks):\n",
    "        print(f\"{i + 1}. {chunk}\\n\")\n",
    "\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    model_query = f\"Briefly explain the {query} with the context {context}\"\n",
    "    generate_text(model_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
